# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o4yMEQb9RTWMaN7UHqfoAMsBhlwapkcR
"""

import os
import pickle
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import load_img, array_to_img, img_to_array
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
import matplotlib.pyplot as plt
from PIL import Image
from google.colab import drive
import re
from tqdm import tqdm

drive.mount('/content/drive')

DL_PROJECT_ROOT = '/content/drive/MyDrive/DeepLearningProject'
extracted_folder_path = os.path.join(DL_PROJECT_ROOT, 'archive(5)')
images_dir = os.path.join(extracted_folder_path, 'Images')
features_path = os.path.join(extracted_folder_path, 'vgg16_features.pkl')

base_model = VGG16(weights='imagenet', include_top=True)
model_vgg = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)

def load_and_preprocess(img_path, target_size=(224,224)):
    img = Image.open(img_path).convert('RGB')
    img = img.resize(target_size)
    arr = img_to_array(img)
    arr = np.expand_dims(arr, axis=0)
    arr = tf.keras.applications.vgg16.preprocess_input(arr)
    return arr

valid_ext = {'.jpg', '.jpeg', '.png', '.bmp'}
image_files = [f for f in os.listdir(images_dir) if os.path.splitext(f.lower())[1] in valid_ext]
features = {}
batch_size = 32
batch_images, batch_ids = [], []

for fname in tqdm(image_files):
    img_path = os.path.join(images_dir, fname)
    arr = load_and_preprocess(img_path)
    batch_images.append(arr)
    batch_ids.append(os.path.splitext(fname)[0])
    if len(batch_images) == batch_size:
        batch_input = np.vstack(batch_images)
        feats = model_vgg.predict(batch_input, verbose=0)
        for idx, img_id in enumerate(batch_ids):
            features[img_id] = feats[idx].reshape(1, -1)
        batch_images, batch_ids = [], []

if batch_images:
    batch_input = np.vstack(batch_images)
    feats = model_vgg.predict(batch_input, verbose=0)
    for idx, img_id in enumerate(batch_ids):
        features[img_id] = feats[idx].reshape(1, -1)

with open(features_path, 'wb') as f:
    pickle.dump(features, f)

from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import load_img, array_to_img
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
import matplotlib.pyplot as plt
from PIL import Image
import re

print("\n--- Loading Pre-extracted Features (Phase 1 Output) ---")
with open(features_path, 'rb') as f:
    features = pickle.load(f)

def load_raw_captions(file_path):
    captions_dict = {}
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    pattern = r'(\S+)\s+(.+)'
    matches = re.findall(pattern, text)
    for id_part_raw, caption in matches:
        if '#' in id_part_raw:
            image_id_base = id_part_raw.split('#')[0]
        else:
            image_id_base = id_part_raw
        image_id = os.path.splitext(image_id_base)[0].strip()
        if image_id not in captions_dict:
            captions_dict[image_id] = []
        captions_dict[image_id].append(caption.strip())
    return captions_dict

caption_folder_path = os.path.join(extracted_folder_path, 'captions')
caption_file_options = [
    os.path.join(caption_folder_path, 'Flickr8k.token.txt'),
    os.path.join(caption_folder_path, 'captions.txt'),
    os.path.join(extracted_folder_path, 'Flickr8k.token.txt'),
    os.path.join(extracted_folder_path, 'captions.txt')
]
for file_option in caption_file_options:
    if os.path.exists(file_option):
        captions_path = file_option
        break

raw_captions = load_raw_captions(captions_path)
synced_captions = {}
for feature_id in features.keys():
    if feature_id in raw_captions:
        synced_captions[feature_id] = raw_captions[feature_id]
        continue
    if feature_id.lower() in raw_captions:
        synced_captions[feature_id] = raw_captions[feature_id.lower()]
        continue
raw_captions = synced_captions
valid_ids = list(raw_captions.keys())
features = {img_id: features[img_id] for img_id in valid_ids}

all_captions = []
for image_id, caption_list in raw_captions.items():
    cleaned_captions = []
    for caption in caption_list:
        caption = caption.lower()
        caption = ''.join(c for c in caption if c.isalpha() or c.isspace())
        caption = ' '.join([word for word in caption.split() if len(word) > 1])
        cleaned_captions.append('startseq ' + caption + ' endseq')
    raw_captions[image_id] = cleaned_captions
    all_captions.extend(cleaned_captions)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1
max_length = max(len(s) for s in tokenizer.texts_to_sequences(all_captions))

def data_generator(data_keys, features, tokenizer, max_length, vocab_size, batch_size):
    X1, X2, y = list(), list(), list()
    n = 0
    while 1:
        for image_id in data_keys:
            n += 1
            image_feature = features[image_id][0]
            caption_list = raw_captions[image_id]
            for caption in caption_list:
                sequence = tokenizer.texts_to_sequences([caption])[0]
                for i in range(1, len(sequence)):
                    in_sequence = sequence[:i]
                    out_word = sequence[i]
                    in_sequence = pad_sequences([in_sequence], maxlen=max_length)[0]
                    out_word = to_categorical([out_word], num_classes=vocab_size)[0]
                    X1.append(image_feature)
                    X2.append(in_sequence)
                    y.append(out_word)
            if n == batch_size:
                yield ((np.array(X1), np.array(X2)), np.array(y))
                X1, X2, y = list(), list(), list()
                n = 0

image_ids = list(features.keys())
np.random.shuffle(image_ids)
train_ids = image_ids[:int(len(image_ids) * 0.8)]
val_ids = image_ids[int(len(image_ids) * 0.8):]
batch_size = 64
steps_per_epoch = len(train_ids) // batch_size
validation_steps = len(val_ids) // batch_size

output_signature_data = (
    (
        tf.TensorSpec(shape=(None, 4096), dtype=tf.float32),
        tf.TensorSpec(shape=(None, max_length), dtype=tf.float32)
    ),
    tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32)
)

train_dataset = tf.data.Dataset.from_generator(
    lambda: data_generator(train_ids, features, tokenizer, max_length, vocab_size, batch_size),
    output_signature=output_signature_data
)

validation_dataset = tf.data.Dataset.from_generator(
    lambda: data_generator(val_ids, features, tokenizer, max_length, vocab_size, batch_size),
    output_signature=output_signature_data
)

input_img = Input(shape=(4096,))
fe1 = Dropout(0.5)(input_img)
fe2 = Dense(256, activation='relu')(fe1)
input_seq = Input(shape=(max_length,))
se1 = Embedding(vocab_size, 256, mask_zero=True)(input_seq)
se2 = Dropout(0.5)(se1)
se3 = LSTM(256)(se2)
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)
model = Model(inputs=[input_img, input_seq], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(train_dataset, epochs=3, steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps)

def word_for_id(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

def generate_caption(model, tokenizer, image_feature, max_length):
    in_text = 'startseq'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        yhat = model.predict([image_feature, sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = word_for_id(yhat, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'endseq':
            break
    final_caption = in_text.replace('startseq', '').replace('endseq', '').strip()
    return final_caption

vgg_model = VGG16()
vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)

plt.figure(figsize=(15, 6))
demo_ids = val_ids[:3]
for i, image_id in enumerate(demo_ids):
    image_path = os.path.join(images_dir, f"{image_id}.jpg")
    image_feature = features[image_id].reshape(1, 4096)
    predicted_caption = generate_caption(model, tokenizer, image_feature, max_length)
    img = Image.open(image_path)
    plt.subplot(1, 3, i + 1)
    plt.imshow(img)
    plt.title(f"Predicted:\n{predicted_caption}", fontsize=10)
    plt.axis('off')
plt.tight_layout()
plt.show()